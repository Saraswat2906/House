# -*- coding: utf-8 -*-
"""HousePrice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E1VUOvm3tdWI8CHzRQWZvpqvlAug0IFE

<H1>  Install the packages
"""

!pip install pandas numpy seaborn matplotlib pycaret

!pip install streamlit

!streamlit run your_app.py

"""<H1> Import the packages"""

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
import pickle
import tensorflow as tf
import streamlit as st
from pycaret.classification import *

from tensorflow.keras.models import Sequential
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback
from tensorflow.keras.models import load_model

"""<H2>Install Tensorflow package"""

pip install tensorflow

"""<H1> Load the data"""

data=pd.read_csv('Raw_Housing_Prices.csv')
data = data.fillna(10000)  # Replace 'value' with the value you want to use for filling NaN.

data

print(data.describe())

sns.distplot(data['Sale Price'])

plt.hist(data['Sale Price'], bins=20)
plt.show()

cols_to_drop=['Waterfront View','No of Times Visited','Condition of the House','Overall Grade','Renovated Year','Date House was Sold']
data=data.drop(cols_to_drop, axis=1)

sns.heatmap(data.corr(),annot = True)

"""#Check missing Data"""

data.isna().sum()

"""#Data Normalisation"""

data = data.iloc[:,1:]
data_norm = (data - data.mean()) / data.std()
data_norm.head()

y_mean = data['Sale Price'].mean()
y_std = data['Sale Price'].std()

def convert_label_value(pred): # Function  will convert the label values back to the original distribution and return it
    return int(pred * y_std + y_mean)

"""#Creating Training and Test Sets

###Selecting Features
"""

X = data_norm.iloc[:, 1:14]
X.head()

"""###Select label"""

Y = data_norm.iloc[:, 0]
Y.head()

X_arr = X.values
Y_arr = Y.values

print('X_arr shape: ', X_arr.shape)
print('Y_arr shape: ', Y_arr.shape)

"""# Split the data into training and testing sets"""

X_train, X_test, Y_train, Y_test = train_test_split(X_arr, Y_arr, test_size=0.2, random_state=42)

"""# Build the neural network model"""

model = Sequential()

model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='linear'))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')
model.summary()

"""# Train the model"""

early_stopping = EarlyStopping(monitor='val_loss', patience=10) # Define early stopping to prevent overfitting

history = model.fit(
    X_train, Y_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_test, Y_test),
    callbacks=[early_stopping]
)

# Evaluate the model on the test set
loss = model.evaluate(X_test, Y_test)
print(f'Mean Squared Error on Test Set: {loss}')
model.save('house_price_model.h5') # Save the trained model

"""#Plot Training and Validation Loss"""

plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""#Final Prediction

###Plotting Raw Predictions:
"""

preds_on_trained = model.predict(X_test)
compare_predictions(preds_on_untrained, preds_on_trained, Y_test)

"""###Plot Price Predictions:"""

price_on_untrained = [convert_label_value(y) for y in preds_on_untrained]
price_on_trained = [convert_label_value(y) for y in preds_on_trained]
price_y_test = [convert_label_value(y) for y in Y_test]

compare_predictions(price_on_untrained, price_on_trained, price_y_test)

st.title('House Price Prediction App')

# Sidebar with input fields
st.sidebar.header('Input Features')

# Create input fields for the features you used in the model
feature_names = ['Bedrooms', 'Bathrooms', 'Sqft Living', 'Sqft Lot', 'Floors', 'View',
                 'Grade', 'Sqft Above', 'Sqft Basement', 'Yr Built', 'Lat', 'Long']

input_features = []
for feature_name in feature_names:
    input_features.append(st.sidebar.number_input(feature_name, value=0))

# Button to make predictions
if st.sidebar.button('Predict Price'):
    input_data = pd.DataFrame([input_features], columns=feature_names)
    predicted_price = predict_price(input_data)
    st.subheader(f'Predicted Price: ${predicted_price:,}')

# Display raw data
st.header('Raw Housing Prices Data')
st.write(data)

# Display a plot
# Display a plot
st.header('Distribution of Sale Price')
plt.figure(figsize=(10, 6))
sns.distplot(data['Sale Price'])
st.pyplot()

# Display heatmap
# Display correlation heatmap
st.header('Correlation Heatmap')

# Create a seaborn heatmap figure
heatmap_fig, ax = plt.subplots(figsize=(10, 6))
sns.heatmap(data.corr(), annot=True, ax=ax)




# Display the model summary
st.header('Neural Network Model Summary')
st.code(model.summary())

# Display training and validation loss plot
st.header('Training and Validation Loss')
st.pyplot(plt.figure(figsize=(10, 6)))
st.pyplot(plt.plot(history.history['loss'], label='Training Loss'))
st.pyplot(plt.plot(history.history['val_loss'], label='Validation Loss'))
st.pyplot(plt.title('Training and Validation Loss'))
st.pyplot(plt.xlabel('Epoch'))
st.pyplot(plt.ylabel('Loss'))
st.pyplot(plt.legend())

st.sidebar.title('Input Features')
feature_names = ['Bedrooms', 'Bathrooms', 'Sqft Living', 'Sqft Lot', 'Floors', 'View',
                 'Grade', 'Sqft Above', 'Sqft Basement', 'Yr Built', 'Lat', 'Long']
input_features = [st.sidebar.number_input(feature_name, value=0) for feature_name in feature_names]

# Button to make predictions
if st.sidebar.button('Predict Price'):
    input_data = pd.DataFrame([input_features], columns=feature_names)
    predicted_price = predict_price(input_data, data, y_std, y_mean)
    st.subheader(f'Predicted Price: ${predicted_price:,}')

# Display raw data
st.header('Raw Housing Prices Data')
st.write(data)

# Display distribution of Sale Price
st.header('Distribution of Sale Price')
plt.figure(figsize=(10, 6))
sns.distplot(data['Sale Price'])
st.pyplot()

# Display correlation heatmap
st.header('Correlation Heatmap')

# Create a seaborn heatmap figure
heatmap_fig, ax = plt.subplots(figsize=(10, 6))
sns.heatmap(data.corr(), annot=True, ax=ax)

# Display the heatmap using st.pyplot
st.pyplot(heatmap_fig)

# Display neural network model summary
st.header('Neural Network Model Summary')
st.header('Neural Network Model Summary')
st.text(model.summary())

# Display training and validation loss plot
st.header('Training and Validation Loss')
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
st.pyplot()
!streamlit run app.py

streamlit run .py